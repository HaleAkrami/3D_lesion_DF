{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5324510-9f87-40e5-837e-ef80aa7a7c20",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import tempfile\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# MONAI Libraries\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Custom Libraries\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "from dataloader import Train ,Eval\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "# Configuration\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "JUPYTER_ALLOW_INSECURE_WRITES=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a60634-4fd9-4a0e-b9a7-75cb47181a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.multitest as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62806039-804d-405c-9f44-9be5d36fd115",
   "metadata": {},
   "source": [
    "# Set seeds and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Configuration\n",
    "config = {\n",
    "    'batch_size': 1,\n",
    "    'imgDimResize': (160, 192, 160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True,\n",
    "    'perc_low': 1,\n",
    "    'perc_high': 99,\n",
    "    'rescaleFactor': 2,\n",
    "    'base_path': '',\n",
    "}\n",
    "\n",
    "# Seed and Device Configuration\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CUDA and CUDNN Configuration\n",
    "# Uncomment the following line to specify CUDA_VISIBLE_DEVICES\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,5,6'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# SimpleITK Configuration\n",
    "# Set the default number of threads and global behavior for SimpleITK\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407f651-439f-4d49-902c-6e74ef45bcef",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c906fe-9038-4e0f-bf7a-9164fe394335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgpath = {}\n",
    "# '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv'\n",
    "#'/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/IXI_train_fold0.csv',\n",
    "#csvpath_trains = ['/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv', '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv']\n",
    "csvpath_trains=['/acmenas/hakrami/3D_lesion_DF/Data/splits/combined_4datasets.csv']\n",
    "pathBase = '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data_train'\n",
    "csvpath_val = '/acmenas/hakrami/3D_lesion_DF/splits/IXI_train_fold0.csv'\n",
    "csvpath_test = '/acmenas/hakrami/3D_lesion_DF/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "# %%\n",
    "\n",
    "\n",
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "# if cfg.mode == 't2':\n",
    "#     keep_t2 = pd.read_csv(cfg.path.IXI.keep_t2) # only keep t2 images that have a t1 counterpart\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = ''\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "    # if cfg.mode == 't2': \n",
    "    #     var_csv[state] =var_csv[state][var_csv[state].img_name.isin(keep_t2['0'].str.replace('t2','t1'))]\n",
    "    #     var_csv[state]['img_path'] = var_csv[state]['img_path'].str.replace('t1','t2')\n",
    "    \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "\n",
    "#data_train = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_train_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "train_loader = DataLoader(data_train, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_val = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_val_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_test = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/Brats21_test.csv', converters={'img_path': pd.eval}), config)\n",
    "test_loader = DataLoader(data_test, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[256, 256, 512],\n",
    "    attention_levels=[False, False, True],\n",
    "    num_head_channels=[0, 0, 512],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model_filename = '/acmenas/hakrami/3D_lesion_DF/models/norm3/model_large_epoch349.pt'\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_filename)) \n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539804e-1fb1-4893-bb59-d93c609a23dc",
   "metadata": {},
   "source": [
    "# Generate an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38579b26-a180-42eb-a885-50d74b3b5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test = next(iter(test_loader))\n",
    "print(sub_test.keys())\n",
    "print(sub_test['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f4096b-b852-481c-b3df-7eca5bb689f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expand the dimensions of sub_test['peak'] to make it [1, 1, 1, 1, 4]\n",
    "peak_expanded = (sub_test['peak'].unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)).long()\n",
    "# Move both tensors to the device\n",
    "image_array = sub_test['vol']['data'].to(device)\n",
    "seg = sub_test['seg']['data'].to(device)\n",
    "peak_expanded = peak_expanded.to(device)\n",
    "image_array = (image_array / peak_expanded)\n",
    "middle_slice_idx = image_array.size(-1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee80b0-63cc-40f5-bfd4-b31dac5d0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_array[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Outlier Image')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0f6b8-ee99-4404-99df-58ac6606b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Segmentation Image')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38724c9b",
   "metadata": {},
   "source": [
    "# Denoise test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13700c-a3bc-4792-aaff-adcb64ab8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(noised_img,sample_time,scheduler,inferer,model):\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=True):\n",
    "            for t in range(sample_time - 1, -1, -1):\n",
    "                batch_size = noised_img.size(0)  # Get the batch size\n",
    "                t_batch=torch.Tensor((t,)).to(noised_img.device)\n",
    "                t_batch = t_batch.unsqueeze(0).expand(batch_size, -1)  # Expand tensor `t` to have the desired batch size\n",
    "                t_batch = t_batch.to(noised_img.device)[:,0] \n",
    "                model_output = model(noised_img, timesteps=t_batch)\n",
    "                noised_img, _ = scheduler.step(model_output, t, noised_img)\n",
    "            return noised_img\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941230d9-7271-485b-b57d-88900f8ddc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_epistemic(noised_img,sample_time,scheduler,inferer,model):\n",
    "    max_sample_time= sample_time[-1]\n",
    "    current_time = sample_time[-2]\n",
    "    i=-2\n",
    "    noised_img_org = noised_img.clone()\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=True):\n",
    "            for t in range(max_sample_time - 1, -1, -1):\n",
    "                batch_size = noised_img.size(0)  # Get the batch size\n",
    "                t_batch=torch.Tensor((t,)).to(noised_img.device)\n",
    "                t_batch = t_batch.unsqueeze(0).expand(batch_size, -1)  # Expand tensor `t` to have the desired batch size\n",
    "                t_batch = t_batch.to(noised_img.device)[:,0] \n",
    "                model_output = model(noised_img, timesteps=t_batch)\n",
    "                noised_img, _ = scheduler.step(model_output, t, noised_img)\n",
    "                if t == current_time:\n",
    "                    noised_img [i,:,:,:,:]= noised_img_org[i,:,:,:,:]\n",
    "                    i-=1\n",
    "                    if i != -6: #TODO hard coded\n",
    "                        current_time = sample_time[i]\n",
    "            return noised_img\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226c1f8-e9a4-4a1d-9c3f-820a30602576",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # For example, if you want to repeat it 5 times\n",
    "\n",
    "# Repeat the tensor\n",
    "repeated_image_array = image_array.repeat(n, 1, 1, 1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9e3ef-a054-4da3-b1ac-80c74910d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sample_time = [100,400,600,800,999]\n",
    "noise =torch.randn_like(repeated_image_array)\n",
    "noisy_img = scheduler.add_noise(original_samples=repeated_image_array, noise=noise, timesteps=torch.tensor(sample_time))\n",
    "noised_img = noisy_img.to(device)\n",
    "middle_slice_idx = repeated_image_array.size(-1) // 2\n",
    "denoised_sample = denoise_epistemic(noisy_img,sample_time,scheduler,inferer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975871f-03e3-44dd-91e3-6ffd75187f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Original Image\n",
    "axes[0, 0].imshow(repeated_image_array[i][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "\n",
    "# Original Image\n",
    "axes[0, 1].imshow(noised_img[i][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 1].set_title('noised Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "\n",
    "\n",
    "# Calculate mean and variance along dimension zero\n",
    "mean_denoised = torch.mean(denoised_sample[:,0][:,:,:].squeeze(), dim=0).cpu().numpy()\n",
    "variance_denoised = torch.std(denoised_sample[:,0][:,:,:].squeeze(), dim=0).cpu().numpy()\n",
    "# Original Image\n",
    "axes[1, 0].imshow(mean_denoised[:,:,middle_slice_idx].squeeze(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[1, 0].set_title('mean denoised Image')\n",
    "\n",
    "\n",
    "# Original Image\n",
    "axes[1, 1].imshow(variance_denoised[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "axes[1, 1].set_title('std denoised Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31916ec6-6383-4e4e-9fb6-475404a69a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "sample_time = 999\n",
    "noise =torch.randn_like(repeated_image_array)\n",
    "noisy_img = scheduler.add_noise(original_samples=repeated_image_array, noise=noise, timesteps=torch.tensor(sample_time))\n",
    "noised_img = noisy_img.to(device)\n",
    "middle_slice_idx = repeated_image_array.size(-1) // 2\n",
    "\n",
    "denoised_sample = denoise(noisy_img,sample_time,scheduler,inferer,model)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f2d54-8fb9-4963-970b-96b71b196465",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "# Original Image\n",
    "axes[0, 0].imshow(repeated_image_array[i][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "\n",
    "# Calculate mean and variance along dimension zero\n",
    "mean_denoised = torch.mean(denoised_sample[:,0][:,:,:].squeeze(), dim=0).cpu().numpy()\n",
    "variance_denoised = torch.std(denoised_sample[:,0][:,:,:].squeeze(), dim=0).cpu().numpy()\n",
    "\n",
    "error = np.abs(repeated_image_array[i][0].squeeze().cpu().numpy()-mean_denoised)\n",
    "\n",
    "# Original Image\n",
    "axes[0, 1].imshow(error[:,:,middle_slice_idx].squeeze(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 1].set_title('noised Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "\n",
    "# Original Image\n",
    "axes[1, 0].imshow(mean_denoised[:,:,middle_slice_idx].squeeze(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[1, 0].set_title('mean denoised Image')\n",
    "\n",
    "\n",
    "# Original Image\n",
    "axes[1, 1].imshow(variance_denoised[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "axes[1, 1].set_title('std denoised Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e07172-c8c1-49f0-b6e7-3ac4be9f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import statsmodels.stats.multitest as mt\n",
    "\n",
    "# Sample 3D tensor\n",
    "x = torch.randn((5, 5, 5))\n",
    "\n",
    "# Compute z-scores\n",
    "x =repeated_image_array[0,0,:,:,:].squeeze().cpu().numpy() \n",
    "z_scores = (error)/ (variance_denoised+0.000001)\n",
    "\n",
    "p_values = 2 * (1 - norm.cdf(np.abs(z_scores)))\n",
    "\n",
    "# # FDR Correction (Benjamini-Hochberg)\n",
    "# def fdr_correction(p_values, alpha=0.05):\n",
    "#     m = p_values.size\n",
    "#     sorted_indices = np.argsort(p_values)\n",
    "#     sorted_p_values = p_values[sorted_indices]\n",
    "#     sorted_q_values = np.zeros(m)\n",
    "#     prev_bh_value = 0\n",
    "\n",
    "#     for i, p in enumerate(sorted_p_values):\n",
    "#         bh_value = (p * m) / (i + 1)\n",
    "#         sorted_q_values[i] = min(bh_value, prev_bh_value)\n",
    "#         prev_bh_value = sorted_q_values[i]\n",
    "\n",
    "#     q_values = np.zeros(m)\n",
    "#     q_values[sorted_indices] = sorted_q_values\n",
    "\n",
    "#     return q_values\n",
    "\n",
    "# q_values = fdr_correction(p_values.flatten())\n",
    "\n",
    "# # Reshape q-values back to the original tensor shape\n",
    "# q_values_tensor = torch.tensor(q_values.reshape(x.shape))\n",
    "\n",
    "\n",
    "p_values_flattened = p_values.flatten()\n",
    "reject, q_values_corrected, _, _ = mt.multipletests(p_values_flattened, method='fdr_bh')\n",
    "q_values_tensor_corrected = torch.tensor(q_values_corrected.reshape(x.shape))\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(10, 10))\n",
    "axes[0, 0].imshow(x[:,:,middle_slice_idx].squeeze(),cmap='gray')\n",
    "axes[0, 0].set_title('input')\n",
    "\n",
    "axes[0, 1].imshow(z_scores[:,:,middle_slice_idx].squeeze(),cmap='gray')\n",
    "axes[0, 1].set_title('z-score')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "# Original Image\n",
    "axes[1, 0].imshow(p_values[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "axes[1, 0].set_title('p-values')\n",
    "\n",
    "\n",
    "axes[1, 1].imshow(q_values_tensor_corrected[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "axes[1, 1].set_title('q-values')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289ab77-7494-43d7-9c41-5c0d5d5b6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming p_values is a 3D numpy array containing p-values\n",
    "alpha = 0.01\n",
    "outlier_image = np.where(q_values_tensor_corrected< alpha, 1, 0)\n",
    "\n",
    "# Plotting the middle slice as before\n",
    "middle_slice_idx = outlier_image.shape[2] // 2\n",
    "plt.imshow(outlier_image[:,:,middle_slice_idx], cmap='gray')\n",
    "plt.title('Outlier Image')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012c29d3-9800-418c-83af-169f7727cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_time = 500\n",
    "noisy_img_500 = scheduler.add_noise(original_samples=repeated_image_array, noise=noise, timesteps=torch.tensor(sample_time))\n",
    "noised_img_500 = noisy_img_500.to(device)\n",
    "middle_slice_idx = repeated_image_array.size(-1) // 2\n",
    "\n",
    "denoised_sample_500 = denoise(noisy_img_500,sample_time,scheduler,inferer,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ecaae-577e-4915-b1ed-3ad2e2f48aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "threshold = 0.2618\n",
    "error_single = np.abs(repeated_image_array[i][0].squeeze().cpu().numpy()-denoised_sample_500[i][0][:,:,:].squeeze().cpu().numpy())\n",
    "binary_mask = (error_single> threshold).astype(np.float32)\n",
    "plt.imshow(binary_mask[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5248c-18c3-4ea2-a19a-6ad03e0424e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.417\n",
    "error_single = np.abs(repeated_image_array[i][0].squeeze().cpu().numpy()-denoised_sample[i][0][:,:,:].squeeze().cpu().numpy())\n",
    "binary_mask = (error_single> threshold).astype(np.float32)\n",
    "plt.imshow(binary_mask[:,:,middle_slice_idx].squeeze(), cmap='gray')\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a82c6-c395-482b-97f5-2d88ba3dfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import median_filter\n",
    "# # Convert tensor to numpy\n",
    "# outlier_image_median = outlier_image.numpy()\n",
    "\n",
    "# # Apply median filter\n",
    "# filtered_array = median_filter(outlier_image_median, size=3)\n",
    "\n",
    "# # Convert back to tensor\n",
    "# plt.imshow(filtered_array[:,:,middle_slice_idx], cmap='gray')\n",
    "# plt.title('Outlier Image filtered')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa1abd-220f-47f6-a94e-ffb9e35dce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've already calculated z_scores as shown in the previous answers:\n",
    "# z_scores = (abs(x - mean_denoised)) / variance_denoised\n",
    "\n",
    "plt.hist(q_values_tensor_corrected.flatten(), bins=30, color='blue', alpha=0.7)\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Z-Scores')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433b7b4-8bca-4601-b94c-4128a07267e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "def dilate_3d_scipy(binary_mask, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Dilate a 3D binary mask using a cubic structuring element.\n",
    "\n",
    "    Parameters:\n",
    "        binary_mask (numpy.ndarray): 3D binary mask to be dilated.\n",
    "        kernel_size (int): Size of the cubic structuring element. Default is 3.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Dilated 3D binary mask.\n",
    "    \"\"\"\n",
    "    # Define a 3D structuring element\n",
    "    kernel = np.ones((kernel_size, kernel_size, kernel_size), bool)\n",
    "    \n",
    "    # Dilate (expand) the binary mask\n",
    "    dilated = binary_dilation(binary_mask, structure=kernel)\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assume 'binary_mask_3d' is your 3D binary mask\n",
    "# binary_mask_3d = ...\n",
    "\n",
    "# Dilate the mask\n",
    "dilated_mask = dilate_3d_scipy(outlier_image, kernel_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8649a-f20d-40ec-9159-0b0a02dd7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dilated_mask[:,:,middle_slice_idx], cmap='gray', vmin=0, vmax=1)\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ff68e-f1e1-4588-b913-870a12601e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "mask = torch.from_numpy(dilated_mask.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "image_array_1= repeated_image_array[0:1,:,:,:,:].to(device)\n",
    "denosied_image_1 = torch.from_numpy(mean_denoised).unsqueeze(0).unsqueeze(0).to(device)\n",
    "#mask = torch.rand_like(image_array_1).to(device)\n",
    "print()\n",
    "\n",
    "val_image_masked = image_array_1.to(device)*(1-mask)+(mask)*denosied_image_1\n",
    "val_image_inpainted = denosied_image_1.to(device) \n",
    "# plot\n",
    "print(val_image_inpainted.shape)\n",
    "scheduler.set_timesteps(num_inference_steps=999)\n",
    "\n",
    "timesteps =50\n",
    "num_resample_steps = 10\n",
    "progress_bar = tqdm.tqdm(reversed(range(timesteps)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        for t in progress_bar:\n",
    "            for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                if t > 0:\n",
    "                    noise =  torch.randn_like(image_array_1).to(device)\n",
    "                    timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "                    val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                        original_samples=val_image_masked, noise=noise, timesteps=timesteps_prev\n",
    "                    )\n",
    "                else:\n",
    "                    val_image_inpainted_prev_known = val_image_masked\n",
    "\n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                if t > 0:\n",
    "                    timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                    model_output = model(val_image_inpainted, timesteps=timesteps)\n",
    "                    val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "\n",
    "                # combine known and unknown using the mask\n",
    "                val_image_inpainted = torch.where(\n",
    "                    mask == 0, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# num_resample_steps = 4\n",
    "# with torch.no_grad():\n",
    "#     with autocast(enabled=True):\n",
    "#         for t in progress_bar:\n",
    "#             for u in range(num_resample_steps):\n",
    "#                 # get the known portion at t-1\n",
    "#                 if t > 0:\n",
    "#                     noise =  torch.randn_like(image_array_1).to(device)\n",
    "#                     timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "#                     val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "#                         original_samples=val_image_masked_2, noise=noise, timesteps=timesteps_prev\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     val_image_inpainted_prev_known = val_image_masked_2\n",
    "\n",
    "#                 # perform a denoising step to get the unknown portion at t-1\n",
    "#                 if t > 0:\n",
    "#                     timesteps = torch.Tensor((t,)).to(device).long()\n",
    "#                     model_output = model(val_image_inpainted_2, timesteps=timesteps)\n",
    "#                     val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted_2)\n",
    "\n",
    "#                 # combine known and unknown using the mask\n",
    "#                 val_image_inpainted_2 = torch.where(\n",
    "#                     mask_2 == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "#                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cf62f-b17d-4778-b467-92aeccc23398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(val_image_masked[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16c114-47f7-4a71-b799-b218450dc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(val_image_inpainted[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b7f6a-86af-4a59-b93b-f4a85c772ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(denoised_sample_500[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10521edb-8d3e-4a89-8f3d-01ff8f696ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denoised_sample[0,0][:,:,middle_slice_idx].squeeze().cpu().numpy(), cmap='gray', vmin=0, vmax=2)\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbff9d-24ac-4a8f-a8d1-644c36c4da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.abs(val_image_inpainted[0,0][:,:,middle_slice_idx]-repeated_image_array[0,0][:,:,middle_slice_idx]).squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.title('Outlier Image single threshold')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefa06e-e2b8-42d5-a1f4-728364b62608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Save numpy array as .nii image\n",
    "nii_img = nib.Nifti1Image(val_image_inpainted[0,0].cpu().numpy(), affine=np.eye(4))\n",
    "nib.save(nii_img, 'sample_image_real_inpaint.nii')\n",
    "\n",
    "\n",
    "\n",
    "# Load the .nii image\n",
    "loaded_img = nib.load('sample_image_real_inpaint.nii')\n",
    "loaded_data = loaded_img.get_fdata()\n",
    "\n",
    "# Plot a slice from the loaded data\n",
    "slice_idx = loaded_data.shape[2] // 2\n",
    "plt.imshow(loaded_data[:, :, slice_idx], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Slice {slice_idx}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1fd20-e6d6-4101-a666-23a69b7dd076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy array as .nii image\n",
    "nii_img = nib.Nifti1Image(denoised_sample_500[0,0].cpu().numpy(), affine=np.eye(4))\n",
    "nib.save(nii_img, 'sample_image_real_500.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c8e0e-e5ca-4345-883b-5ac34c25b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save numpy array as .nii image\n",
    "nii_img = nib.Nifti1Image(denoised_sample[0,0].cpu().numpy(), affine=np.eye(4))\n",
    "nib.save(nii_img, 'sample_image_real_999.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbae32d-e5c1-422c-a08e-9c09b6aea459",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_array = sub_test['path_normal']['data'].to(device)\n",
    "healthy_array = (healthy_array / peak_expanded)\n",
    "nii_img = nib.Nifti1Image(healthy_array[0,0].cpu().numpy(), affine=np.eye(4))\n",
    "nib.save(nii_img, 'sample_image_real_healthy.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c52a15-bf1d-4a8d-918d-3490850d8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nii_img = nib.Nifti1Image(repeated_image_array[0,0].cpu().numpy(), affine=np.eye(4))\n",
    "nib.save(nii_img, 'sample_image_real_lesion.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe6e45-3353-48ef-bc51-6cc343048c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ab3cb-4a60-4d4a-8d48-973050368323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:percent,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
