==========================================
SLURM_JOB_ID = 16578639
SLURM_JOB_NODELIST = b09-12
TMPDIR = /tmp/SLURM_16578639
==========================================
wandb: Currently logged in as: hale-akrami (usc_akrami). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.9
wandb: Run data is saved locally in /project/ajoshi_27/akrami/3D_lesion_DF/wandb/run-20230903_010239-ylbsio25
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run full_res
wandb: ‚≠êÔ∏è View project at https://wandb.ai/usc_akrami/33_ddpm
wandb: üöÄ View run at https://wandb.ai/usc_akrami/33_ddpm/runs/ylbsio25
2023-09-03 01:03:02,482 - A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
  0%|                                          | 0/97 [00:00<?, ?it/s]Epoch 0:   0%|                                 | 0/97 [00:00<?, ?it/s]Epoch 0:   0%|                                 | 0/97 [00:18<?, ?it/s]
Traceback (most recent call last):
  File "/project/ajoshi_27/akrami/3D_lesion_DF/3d_ddpm.py", line 191, in <module>
    noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)
  File "/project/ajoshi_27/akrami/3D_lesion_DF/generative/inferers/inferer.py", line 59, in __call__
    prediction = diffusion_model(x=noisy_image, timesteps=timesteps, context=condition)
  File "/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/project/ajoshi_27/akrami/3D_lesion_DF/generative/networks/nets/diffusion_model_unet.py", line 1917, in forward
    h = upsample_block(hidden_states=h, res_hidden_states_list=res_samples, temb=emb, context=context)
  File "/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/project/ajoshi_27/akrami/3D_lesion_DF/generative/networks/nets/diffusion_model_unet.py", line 1338, in forward
    hidden_states = attn(hidden_states)
  File "/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/project/ajoshi_27/akrami/3D_lesion_DF/generative/networks/nets/diffusion_model_unet.py", line 448, in forward
    x = self._attention(query, key, value)
  File "/project/ajoshi_27/akrami/3D_lesion_DF/generative/networks/nets/diffusion_model_unet.py", line 414, in _attention
    attention_probs = attention_scores.softmax(dim=-1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.49 GiB (GPU 0; 79.17 GiB total capacity; 71.44 GiB already allocated; 4.19 GiB free; 73.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: üöÄ View run full_res at: https://wandb.ai/usc_akrami/33_ddpm/runs/ylbsio25
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230903_010239-ylbsio25/logs
