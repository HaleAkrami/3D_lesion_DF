{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23f7e62-512c-4fde-96c9-0ad44a7f8211",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhale-akrami\u001b[0m (\u001b[33musc_akrami\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/ajoshi_27/akrami/3D_lesion_DF/wandb/run-20230924_171854-c7qh0ur9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/c7qh0ur9' target=\"_blank\">note_half_bio</a></strong> to <a href='https://wandb.ai/usc_akrami/33_ddpm_bio' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/usc_akrami/33_ddpm_bio' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/c7qh0ur9' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio/runs/c7qh0ur9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-24 17:19:05,523 - A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:c7qh0ur9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">note_half_bio</strong> at: <a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/c7qh0ur9' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio/runs/c7qh0ur9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230924_171854-c7qh0ur9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:c7qh0ur9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/ajoshi_27/akrami/3D_lesion_DF/wandb/run-20230924_171905-kuu0x9lf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/usc_akrami/33_ddpm/runs/kuu0x9lf' target=\"_blank\">ethereal-bush-66</a></strong> to <a href='https://wandb.ai/usc_akrami/33_ddpm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/usc_akrami/33_ddpm' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/usc_akrami/33_ddpm/runs/kuu0x9lf' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm/runs/kuu0x9lf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='33_ddpm_bio',name='note_half_bio')\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# MONAI libraries\n",
    "# from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, \n",
    "    CenterSpatialCropd, \n",
    "    Compose, \n",
    "    Lambdad, \n",
    "    LoadImaged, \n",
    "    Resized, \n",
    "    ScaleIntensityd\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Other medical image processing libraries\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet_2Q\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "\n",
    "# Weights and Biases for experiment tracking\n",
    "from dataloader import Train,Eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'batch_size': 4,\n",
    "    'imgDimResize':(160,192,160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True, \n",
    "    'perc_low': 0, \n",
    "    'perc_high': 100,\n",
    "    'rescaleFactor':2,\n",
    "    'base_path': '/scratch1/akrami/Latest_Data/Data',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902ed921-4982-4f20-a518-fb419aa0aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(config )\n",
    "\n",
    "\n",
    "imgpath = {}\n",
    "# '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv'\n",
    "#'/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/IXI_train_fold0.csv',\n",
    "#csvpath_trains = ['/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv', '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv']\n",
    "csvpath_trains=['./combined.csv']\n",
    "pathBase = '/scratch1/akrami/Data_train'\n",
    "csvpath_val = '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/IXI_val_fold0.csv'\n",
    "csvpath_test = '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "\n",
    "# dfffff=  pd.concat(df_list, ignore_index=True)\n",
    "# dfffff.to_csv(\"./combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff2d01eb-884f-4358-99e8-547d5b781ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "# if cfg.mode == 't2':\n",
    "#     keep_t2 = pd.read_csv(cfg.path.IXI.keep_t2) # only keep t2 images that have a t1 counterpart\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = pathBase  + var_csv[state]['norm_path']\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "    # if cfg.mode == 't2': \n",
    "    #     var_csv[state] =var_csv[state][var_csv[state].img_name.isin(keep_t2['0'].str.replace('t2','t1'))]\n",
    "    #     var_csv[state]['img_path'] = var_csv[state]['img_path'].str.replace('t1','t2')\n",
    "    \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "\n",
    "#data_train = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_train_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "train_loader = DataLoader(data_train, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_val = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_val_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_test = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/Brats21_test.csv', converters={'img_path': pd.eval}), config)\n",
    "test_loader = DataLoader(data_test, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=8)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet_2Q(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[32, 64, 128, 128],\n",
    "    attention_levels=[False, False, False,True],\n",
    "    num_head_channels=[0, 0, 0,32],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acba5f5-6e1e-4ec9-8068-b44b46171b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_filename = '/scratch1/akrami/models/3Ddiffusion/half_3Q_note/model_epoch4.pt'\n",
    "model.load_state_dict(torch.load(model_filename),strict= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4162235d-2328-4815-8952-a5ad593145e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|█▌      | 481/2410 [04:26<17:47,  1.81it/s, loss=0.0193]\n"
     ]
    },
    {
     "ename": "RemoteError",
     "evalue": "\n---------------------------------------------------------------------------\nCaught RemoteError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/project/ajoshi_27/akrami/3D_lesion_DF/dataloader.py\", line 309, in __getitem__\n    self.cache.cache(index, subject)\n  File \"/project/ajoshi_27/akrami/3D_lesion_DF/dataloader.py\", line 292, in cache\n    self._dict[str(key)] = (subject)\n  File \"<string>\", line 2, in __setitem__\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/managers.py\", line 825, in _callmethod\n    raise convert_to_error(kind, result)\nmultiprocessing.managers.RemoteError: \n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/managers.py\", line 245, in serve_client\n    request = recv()\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/connection.py\", line 251, in recv\n    return _ForkingPickler.loads(buf.getbuffer())\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/reduction.py\", line 164, in recvfds\n    raise RuntimeError('received %d items of ancdata' %\nRuntimeError: received 0 items of ancdata\n---------------------------------------------------------------------------\n---------------------------------------------------------------------------",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     42\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     44\u001b[0m    \u001b[38;5;66;03m# images = batch[\"image\"].to(device)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvol\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     46\u001b[0m     images  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m images\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRemoteError\u001b[0m: \n---------------------------------------------------------------------------\nCaught RemoteError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/project/ajoshi_27/akrami/3D_lesion_DF/dataloader.py\", line 309, in __getitem__\n    self.cache.cache(index, subject)\n  File \"/project/ajoshi_27/akrami/3D_lesion_DF/dataloader.py\", line 292, in cache\n    self._dict[str(key)] = (subject)\n  File \"<string>\", line 2, in __setitem__\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/managers.py\", line 825, in _callmethod\n    raise convert_to_error(kind, result)\nmultiprocessing.managers.RemoteError: \n---------------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/managers.py\", line 245, in serve_client\n    request = recv()\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/connection.py\", line 251, in recv\n    return _ForkingPickler.loads(buf.getbuffer())\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/resource_sharer.py\", line 58, in detach\n    return reduction.recv_handle(conn)\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/reduction.py\", line 189, in recv_handle\n    return recvfds(s, 1)[0]\n  File \"/home1/akrami/.conda/envs/working_monai/lib/python3.9/multiprocessing/reduction.py\", line 164, in recvfds\n    raise RuntimeError('received %d items of ancdata' %\nRuntimeError: received 0 items of ancdata\n---------------------------------------------------------------------------\n---------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile=0.5, reduction='mean'):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert preds.size() == target.size()\n",
    "        diff = target - preds\n",
    "        loss = torch.where(diff >= 0, self.quantile * diff, (self.quantile - 1) * diff)\n",
    "        if self.reduction =='mean':\n",
    "            return torch.mean(loss)\n",
    "        else:\n",
    "            return torch.mean(loss, dim=(1, 2, 3))\n",
    "            \n",
    "\n",
    "quantile_loss_l = QuantileLoss(quantile=0.05)\n",
    "quantile_loss_m = QuantileLoss(quantile=0.5)\n",
    "quantile_loss_h = QuantileLoss(quantile=0.95)\n",
    "\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "n_epochs = config.get('n_epochs',100)\n",
    "val_interval =config.get('val_interval',2)\n",
    "\n",
    "wandb.watch(model, log_freq=100)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "       # images = batch[\"image\"].to(device)\n",
    "        images = batch['vol']['data'].to(device)\n",
    "        images  = torch.rand(1).item()* images\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            prediction,prediction_m,prediction_h = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "            loss = quantile_loss_l(prediction, images) + quantile_loss_m(prediction_m, images) + quantile_loss_h(prediction_h, images)\n",
    "\n",
    "            #loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "    wandb.log({\"loss_train\": epoch_loss / (step + 1)},step=epoch)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            images = batch['vol']['data'].to(device)\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    timesteps = torch.randint(\n",
    "                        0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    prediction,prediction_m,prediction_h = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = quantile_loss_l(prediction, images) + quantile_loss_m(prediction_m, images) + quantile_loss_h(prediction_h, images)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        wandb.log({\"loss_val\": val_epoch_loss / (step + 1)},step=epoch)\n",
    "\n",
    "        # Sampling image during training\n",
    "        #80, 96, 80\n",
    "        image = torch.randn_like(images)[0:1,:,:,:]\n",
    "        image = image.to(device)\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "        timesteps = torch.randint(\n",
    "                        inferer.scheduler.num_train_timesteps-1, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "        with autocast(enabled=True):\n",
    "            prediction,prediction_m,prediction_h = inferer(inputs=image, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "\n",
    "\n",
    "        middle_slice_idx = int(image.size(-1) // 2)\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction_m[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_m\": [wandb.Image(plt)]},step=epoch)\n",
    "        filename = f\"./results/half_norm_3Q_note_v2/sample_epoch{epoch}.png\"\n",
    "        plt.savefig(filename, dpi=300) \n",
    "\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_l\": [wandb.Image(plt)]},step=epoch)\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction_h[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_h\": [wandb.Image(plt)]},step=epoch)\n",
    "        # Modify the filename to include the epoch number\n",
    "        \n",
    "\n",
    "         \n",
    "        # Save the model\n",
    "        model_filename = f\"/scratch1/akrami/models/3Ddiffusion/half_3Q_note_v2/model_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661292b-86c7-4266-8501-da944577e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f\"/scratch1/akrami/models/3Ddiffusion/half_3Q_note/model_epoch{epoch}.pt\"\n",
    "torch.save(model.state_dict(), model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e68be7-6c57-4e0e-9b7a-03e1892e2687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_monai",
   "language": "python",
   "name": "working_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
