{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f1a3f-3528-4eee-a5d8-f164db1a3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# MONAI libraries\n",
    "# from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, \n",
    "    CenterSpatialCropd, \n",
    "    Compose, \n",
    "    Lambdad, \n",
    "    LoadImaged, \n",
    "    Resized, \n",
    "    ScaleIntensityd\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Other medical image processing libraries\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet_2Q\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "from dataloader import Train,Eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'batch_size': 4,\n",
    "    'imgDimResize':(160,192,160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True, \n",
    "    'perc_low': 0, \n",
    "    'perc_high': 100,\n",
    "    'rescaleFactor':2,\n",
    "    'base_path': '',\n",
    "}\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "imgpath = {}\n",
    "csvpath_trains=['./combined.csv']\n",
    "pathBase = '/scratch1/akrami/Data_train'\n",
    "csvpath_val = '/project/ajoshi_27/akrami/3D_lesion_DF/Data/splits/BioBank_val.csv'\n",
    "csvpath_test = '/project/ajoshi_27/akrami/3D_lesion_DF/Data/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "\n",
    "# %%\n",
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "# if cfg.mode == 't2':\n",
    "#     keep_t2 = pd.read_csv(cfg.path.IXI.keep_t2) # only keep t2 images that have a t1 counterpart\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = ''\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "    # if cfg.mode == 't2': \n",
    "    #     var_csv[state] =var_csv[state][var_csv[state].img_name.isin(keep_t2['0'].str.replace('t2','t1'))]\n",
    "    #     var_csv[state]['img_path'] = var_csv[state]['img_path'].str.replace('t1','t2')\n",
    "    \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "\n",
    "#data_train = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_train_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "train_loader = DataLoader(data_train, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_val = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_val_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_test = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/Brats21_test.csv', converters={'img_path': pd.eval}), config)\n",
    "test_loader = DataLoader(data_test, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=8)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0755d6b-ac34-457b-9b75-797bd844fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DiffusionModelUNet_2Q(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[32, 64, 128, 128],\n",
    "    attention_levels=[False, False, False,True],\n",
    "    num_head_channels=[0, 0, 0,32],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905589c-caa6-4352-a35b-1bd1d1d3e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your model filename\n",
    "model_filename ='/project/ajoshi_27/akrami/3D_lesion_DF/models/3Q/model_epoch1298.pt'\n",
    "model.load_state_dict(torch.load(model_filename))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2edce-6577-4706-b17b-2e24f64f4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test = next(iter(test_loader))\n",
    "print(sub_test.keys())\n",
    "print(sub_test['age'])\n",
    "print(sub_test['vol']['data'].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2165c5-eca7-40a3-859a-4f8098ac2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expand the dimensions of sub_test['peak'] to make it [1, 1, 1, 1, 4]\n",
    "peak_expanded = (sub_test['peak'].unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)).long()\n",
    "# Move both tensors to the device\n",
    "image_array = sub_test['vol']['data'].to(device)\n",
    "peak_expanded = peak_expanded.to(device)\n",
    "\n",
    "# Perform the division\n",
    "image_array = (image_array / peak_expanded)\n",
    "middle_slice_idx = image_array.size(-1) // 2\n",
    "# image_array = image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f9f03-3fe5-4f58-877f-2a800f94b7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise = torch.randn_like(image_array).to(device)\n",
    "timesteps = torch.randint(50, 51, (image_array.shape[0],), device=image_array.device).long()\n",
    "_,model_out,_ = model(x=image_array, timesteps=timesteps)\n",
    "prediction,prediction_m,prediction_h = inferer(inputs=image_array, diffusion_model=model, noise=noise, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2c1b0-7ce3-4085-8ede-205f16330bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_slice_idx = int(image_array.size(-1) // 2)\n",
    "i=0\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image_array[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(prediction_m[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(model_out[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(prediction_h[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b067a6d-6600-435c-98ee-ec727fa3c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = image_array * ((image_array < prediction) | (image_array > prediction_h)).float()\n",
    "segmented = segmented*(image_array>0.001)\n",
    "error = torch.abs(image_array-prediction_m)\n",
    "\n",
    "middle_slice_idx = int(image_array.size(-1) // 2)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(segmented[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(error[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(sub_test['seg']['data'][i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7f2c9-f58f-4369-a1ae-e5e2d7937f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.randn_like(image_array).to(device)\n",
    "mask[mask<=0.5] = 0\n",
    "mask[mask>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34240e53-fe18-4cff-be31-5eac8da06d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(mask[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187f5f2-42e8-4ae2-9732-64a96d6d4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 2))\n",
    "masked_image = image_array*mask\n",
    "plt.imshow(masked_image[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd52b12-2ef7-4787-8be0-9375d79bf866",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_masked = image_array.to(device)*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f35f5-a544-4f64-bb2f-4215dd81e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(image_array).to(device)\n",
    "timesteps = torch.randint(50, 51, (image_array.shape[0],), device=image_array.device).long()\n",
    "noise = noise*mask\n",
    "_,model_out,_ = model(x=image_array*mask, timesteps=timesteps)\n",
    "prediction,prediction_m,prediction_h = inferer(inputs=image_array, diffusion_model=model, noise=noise, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec702a68-6ba8-4673-b741-1f113b1a8140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62346539-f055-4d11-ac58-c985ab1cc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_slice_idx = int(image_array.size(-1) // 2)\n",
    "i=0\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(image_array[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(prediction_m[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(model_out[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(prediction_h[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597de60-aee9-4ec9-bf62-7ed0750b6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = image_array * ((image_array < prediction) | (image_array > prediction_h)).float()\n",
    "segmented = segmented*(image_array>0.001)\n",
    "error = torch.abs(image_array-prediction_m)\n",
    "\n",
    "middle_slice_idx = int(image_array.size(-1) // 2)\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(segmented[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(error[i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(sub_test['seg']['data'][i, 0, :, :, middle_slice_idx].detach().cpu().numpy(), vmin=0, vmax=2, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad889a-f996-40e3-9a75-3170fa38f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "mask = mask.to(device)\n",
    "val_image_masked = image_array.to(device)*mask\n",
    "val_image_inpainted = torch.randn_like(image_array).to(device)\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps=500)\n",
    "progress_bar = tqdm.tqdm(scheduler.timesteps)\n",
    "\n",
    "num_resample_steps = 10\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        for t in progress_bar:\n",
    "            for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                if t > 0:\n",
    "                    noise =  torch.randn_like(image_array).to(device)\n",
    "                    timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "                    val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                        original_samples=val_image_masked, noise=noise, timesteps=timesteps_prev\n",
    "                    )\n",
    "                else:\n",
    "                    val_image_inpainted_prev_known = val_image_masked\n",
    "\n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                if t > 0:\n",
    "                    timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                    model_output = model(val_image_inpainted, timesteps=timesteps)\n",
    "                    val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "\n",
    "                # combine known and unknown using the mask\n",
    "                val_image_inpainted = torch.where(\n",
    "                    mask == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                )\n",
    "\n",
    "                # perform resampling\n",
    "                if t > 0 and u < (num_resample_steps - 1):\n",
    "                    # sample x_t from x_t-1\n",
    "                    noise = torch.randn_like(image_array).to(device)\n",
    "                    val_image_inpainted = (\n",
    "                        torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted\n",
    "                        + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                    )\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "# plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e5da0-6944-4f4a-a43b-ee83f7e619f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12084287-4dd7-4acf-b33f-cc00740b6275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155a771-c0d3-4273-8268-c1ea139f21ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_monai",
   "language": "python",
   "name": "working_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
