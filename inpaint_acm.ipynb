{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d21ac-398c-4f84-8fdb-b242fcf225c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4e7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab9bd3-cccf-466c-b3f9-4585938775aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from noise import snoise3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "width, height, depth = 100, 100, 100\n",
    "scale = 0.09 # Adjust this to change the \"frequency\" of the noise\n",
    "threshold = 0.0  # Since Perlin noise returns values between -1 and 1, a threshold of 0 should roughly split the values\n",
    "\n",
    "# Generate 3D Perlin noise\n",
    "noise_array = np.empty((width, height, depth))\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        for z in range(depth):\n",
    "            noise_array[x, y, z] = snoise3(x * scale, y * scale, z * scale)\n",
    "\n",
    "# Threshold the noise\n",
    "thresholded_array = np.where(noise_array > threshold, 1, 0)\n",
    "\n",
    "# For visualization: Show a 2D slice of the 3D data\n",
    "plt.imshow(thresholded_array[:, :, 50], cmap='gray')  # Change 50 to view a different depth slice\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc18016-49bf-4d3e-98e0-5d3188ca7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import tempfile\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "\n",
    "# Third-party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# MONAI Libraries\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Custom Libraries\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "from dataloader import Train ,Eval \n",
    "\n",
    "# Configuration\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "JUPYTER_ALLOW_INSECURE_WRITES=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5db540",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b04c0-2f47-4a72-8e87-58c146322644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Configuration\n",
    "config = {\n",
    "    'batch_size': 1,\n",
    "    'imgDimResize': (160, 192, 160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True,\n",
    "    'perc_low': 1,\n",
    "    'perc_high': 99,\n",
    "    'rescaleFactor': 2,\n",
    "    'base_path': '/scratch1/akrami/Latest_Data/Data',\n",
    "}\n",
    "\n",
    "# Seed and Device Configuration\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CUDA and CUDNN Configuration\n",
    "# Uncomment the following line to specify CUDA_VISIBLE_DEVICES\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,5,6'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# SimpleITK Configuration\n",
    "# Set the default number of threads and global behavior for SimpleITK\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c40b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgpath = {}\n",
    "# '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv'\n",
    "#'/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/IXI_train_fold0.csv',\n",
    "#csvpath_trains = ['/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv', '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv']\n",
    "csvpath_trains=['/acmenas/hakrami/3D_lesion_DF/Data/splits/combined_4datasets.csv']\n",
    "pathBase = '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data_train'\n",
    "csvpath_val = '/acmenas/hakrami/3D_lesion_DF/splits/IXI_train_fold0.csv'\n",
    "csvpath_test = '/acmenas/hakrami/3D_lesion_DF/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "# %%\n",
    "\n",
    "\n",
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "# if cfg.mode == 't2':\n",
    "#     keep_t2 = pd.read_csv(cfg.path.IXI.keep_t2) # only keep t2 images that have a t1 counterpart\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = ''\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "    # if cfg.mode == 't2': \n",
    "    #     var_csv[state] =var_csv[state][var_csv[state].img_name.isin(keep_t2['0'].str.replace('t2','t1'))]\n",
    "    #     var_csv[state]['img_path'] = var_csv[state]['img_path'].str.replace('t1','t2')\n",
    "    \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "\n",
    "#data_train = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_train_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "train_loader = DataLoader(data_train, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_val = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_val_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_test = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/Brats21_test.csv', converters={'img_path': pd.eval}), config)\n",
    "test_loader = DataLoader(data_test, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a1c82-dd40-401a-aaca-b4ee18ae4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[256, 256, 512],\n",
    "    attention_levels=[False, False, True],\n",
    "    num_head_channels=[0, 0, 512],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model_filename = '/acmenas/hakrami/3D_lesion_DF/models/norm3/model_large_epoch349.pt'\n",
    "\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load(model_filename)) \n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7f97f-3780-44ee-a5c5-bc92f5ea0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sub_test = next(iter(val_loader))\n",
    "print(sub_test.keys())\n",
    "print(sub_test['age'])\n",
    "\n",
    "# Expand the dimensions of sub_test['peak'] to make it [1, 1, 1, 1, 4]\n",
    "peak_expanded = (sub_test['peak'].unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)).long()\n",
    "# Move both tensors to the device\n",
    "image_array = sub_test['vol']['data'].to(device)\n",
    "peak_expanded = peak_expanded.to(device)\n",
    "image_array = (image_array / peak_expanded)\n",
    "middle_slice_idx = image_array.size(-1) // 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606721f-b823-4808-9b8c-3bd45305222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor of shape (batch_size, channels, height, width, depth)\n",
    "image_tensor = image_array[0:1,:,:,:,:]+0\n",
    "\n",
    "\n",
    "# Coordinates for the patch you want to remove\n",
    "start_h, start_w, start_d = 30, 30, 30\n",
    "end_h, end_w, end_d = 50, 50, 50\n",
    "\n",
    "\n",
    "\n",
    "# \"Remove\" the patch from the image by setting it to zeros (or any other value)\n",
    "image_tensor[:, :, start_h:end_h, start_w:end_w, start_d:end_d] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abeae22-5668-4038-9144-745dd7540609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "_,_,width, height, depth = image_tensor.size()\n",
    "scale = 0.09 # Adjust this to change the \"frequency\" of the noise\n",
    "threshold = 0.0  # Since Perlin noise returns values between -1 and 1, a threshold of 0 should roughly split the values\n",
    "\n",
    "# Generate 3D Perlin noise\n",
    "noise_array = np.empty((width, height, depth))\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        for z in range(depth):\n",
    "            noise_array[x, y, z] = snoise3(x * scale, y * scale, z * scale)\n",
    "\n",
    "# Threshold the noise\n",
    "mask = np.where(noise_array > threshold, 1, 0)\n",
    "\n",
    "# For visualization: Show a 2D slice of the 3D data\n",
    "plt.imshow(thresholded_array[:, :, middle_slice_idx], cmap='gray')  # Change 50 to view a different depth slice\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a5a1d-df5e-4163-867d-58a5f2e0a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "import torch\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "mask_tensor = torch.tensor(mask).float()\n",
    "\n",
    "# Now move the tensor to the desired device\n",
    "mask= mask_tensor.to(device)\n",
    "import tqdm as tqdm\n",
    "mask = mask.to(device)\n",
    "val_image_masked = image_tensor.to(device)*mask\n",
    "val_image_inpainted = torch.randn_like(val_image_masked).to(device)\n",
    "timesteps = torch.Tensor((999,)).to(mask.device).long()\n",
    "scheduler.set_timesteps(num_inference_steps=999)\n",
    "progress_bar = tqdm.tqdm(scheduler.timesteps)\n",
    "\n",
    "num_resample_steps = 4\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        for t in progress_bar:\n",
    "            for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                if t > 0:\n",
    "                    noise =  torch.randn_like(val_image_masked).to(device)\n",
    "                    timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "                    val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                        original_samples=val_image_masked, noise=noise, timesteps=timesteps_prev\n",
    "                    )\n",
    "                else:\n",
    "                    val_image_inpainted_prev_known = val_image_masked\n",
    "                \n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                if t > 0:\n",
    "                    timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                    model_output = model(val_image_inpainted, timesteps=timesteps)\n",
    "                    val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "\n",
    "                # combine known and unknown using the mask\n",
    "                val_image_inpainted = torch.where(\n",
    "                    mask == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                )\n",
    "\n",
    "                # perform resampling\n",
    "                if t > 0 and u < (num_resample_steps - 1):\n",
    "                    # sample x_t from x_t-1\n",
    "                    noise = torch.randn_like(val_image_masked).to(device)\n",
    "                    val_image_inpainted = (\n",
    "                        torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted\n",
    "                        + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e786fc-8500-484c-abcf-ec8550c9aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes[0, 0].imshow(image_array[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "# Original Image\n",
    "axes[0, 1].imshow(image_tensor[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 1].set_title('noisy Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "# Original Image\n",
    "axes[1, 0].imshow(val_image_masked[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[1, 0].set_title('denoised Image inpaint')\n",
    "\n",
    "axes[1, 1].imshow(val_image_inpainted[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[1, 1].set_title('denoised Image inpaint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163e6b8-9200-4540-9dff-ed74b90b0628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e750b-a31f-4eee-8ceb-f63a7c2061d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = 1-mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9af82-8f4b-4665-b20b-114251dab280",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_masked_1 = val_image_inpainted.to(device)*mask_1\n",
    "val_image_inpainted_1 = torch.randn_like(val_image_masked_1).to(device)\n",
    "timesteps = torch.Tensor((999,)).to(noise.device).long()\n",
    "scheduler.set_timesteps(num_inference_steps=999)\n",
    "progress_bar = tqdm.tqdm(scheduler.timesteps)\n",
    "\n",
    "num_resample_steps = 4\n",
    "with torch.no_grad():\n",
    "    with autocast(enabled=True):\n",
    "        for t in progress_bar:\n",
    "            for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                if t > 0:\n",
    "                    noise =  torch.randn_like(val_image_masked_1).to(device)\n",
    "                    timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "                    val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                        original_samples=val_image_masked_1, noise=noise, timesteps=timesteps_prev\n",
    "                    )\n",
    "                else:\n",
    "                    val_image_inpainted_prev_known = val_image_masked_1\n",
    "                \n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                if t > 0:\n",
    "                    timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                    model_output = model(val_image_inpainted_1, timesteps=timesteps)\n",
    "                    val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted_1)\n",
    "\n",
    "                # combine known and unknown using the mask\n",
    "                val_image_inpainted_1 = torch.where(\n",
    "                    mask_1 == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                )\n",
    "\n",
    "                # perform resampling\n",
    "                if t > 0 and u < (num_resample_steps - 1):\n",
    "                    # sample x_t from x_t-1\n",
    "                    noise = torch.randn_like(val_image_masked_1).to(device)\n",
    "                    val_image_inpainted_1 = (\n",
    "                        torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted_1\n",
    "                        + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a33f62-5a00-4ee1-8748-a4f0f890e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes[0, 0].imshow(image_array[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 0].set_title('Original Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "# Original Image\n",
    "axes[0, 1].imshow(image_tensor[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[0, 1].set_title('noisy Image')\n",
    "#recon_img = denoise(image_array.to(device),sample_time,scheduler,inferer,model)\n",
    "# Original \n",
    "error = torch.abs(val_image_inpainted_1-image_tensor)\n",
    "axes[1, 0].imshow(error[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=1, cmap='gray')\n",
    "axes[1, 0].set_title('denoised Image inpaint')\n",
    "\n",
    "axes[1, 1].imshow(val_image_inpainted_1[0][0][:,:,middle_slice_idx].squeeze().cpu().numpy(), vmin=0, vmax=2, cmap='gray')\n",
    "axes[1, 1].set_title('denoised Image inpaint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa9e84-1bae-40d4-a5f5-fb8f6e7bb7ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_3d_checkerboard(size, block_size):\n",
    "    \"\"\"\n",
    "    Generate a 3D checkerboard mask using PyTorch.\n",
    "\n",
    "    :param size: The size of the 3D tensor (depth, height, width)\n",
    "    :param block_size: Size of each block in the checkerboard\n",
    "    :return: 3D tensor of shape `size`\n",
    "    \"\"\"\n",
    "    \n",
    "    depth, height, width = size\n",
    "    pattern = torch.arange(depth * height * width).reshape(depth, height, width)\n",
    "    checkerboard = ((pattern // block_size) % 2) ^ ((pattern // (block_size * depth)) % 2) ^ ((pattern // (block_size * depth * height)) % 2)\n",
    "    \n",
    "    return checkerboard\n",
    "\n",
    "# Example usage\n",
    "_,_,width, height, depth = image_tensor.size()\n",
    "size = width, height, depth\n",
    "block_size = 2\n",
    "mask = generate_3d_checkerboard(size, block_size)\n",
    "print(mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
