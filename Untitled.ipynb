{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856b6a09-6849-44e6-b67d-ceec9de6287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhale-akrami\u001b[0m (\u001b[33musc_akrami\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/ajoshi_27/akrami/3D_lesion_DF/wandb/run-20231117_001514-sbuzyy6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/usc_akrami/3D_ddpm/runs/sbuzyy6e' target=\"_blank\">test_inpaint_brats</a></strong> to <a href='https://wandb.ai/usc_akrami/3D_ddpm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/usc_akrami/3D_ddpm' target=\"_blank\">https://wandb.ai/usc_akrami/3D_ddpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/usc_akrami/3D_ddpm/runs/sbuzyy6e' target=\"_blank\">https://wandb.ai/usc_akrami/3D_ddpm/runs/sbuzyy6e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 141\u001b[0m\n\u001b[1;32m    138\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m    140\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/project/ajoshi_27/akrami/3D_lesion_DF/models/model_large_epoch999.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 141\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    142\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenoise\u001b[39m(noised_img,sample_time,scheduler,inferer,model):\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:1083\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1079\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1083\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1084\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/.conda/envs/working_monai/lib/python3.9/site-packages/torch/serialization.py:173\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    171\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on CUDA device \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    174\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but torch.cuda.device_count() is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    175\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load with map_location to map your storages \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    176\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto an existing device.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on CUDA device 0 but torch.cuda.device_count() is 0. Please use torch.load with map_location to map your storages to an existing device."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import tempfile\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "import tqdm\n",
    "# Third-party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchio as tio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "# MONAI Libraries\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.utils import set_determinism\n",
    "from monai.losses import MaskedLoss\n",
    "\n",
    "# Custom Libraries\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "from dataloader import Train ,Eval \n",
    "from torch.nn.functional import mse_loss\n",
    "from monai.losses.ssim_loss import SSIMLoss\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Configuration\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "warnings.filterwarnings('ignore')\n",
    "import wandb\n",
    "wandb.init(project='3D_ddpm',name='test_inpaint_brats')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,4\"\n",
    "\n",
    "# Initialize Configuration\n",
    "config = {\n",
    "    'batch_size': 1,\n",
    "    'imgDimResize': (160, 192, 160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True,\n",
    "    'perc_low': 1,\n",
    "    'perc_high': 99,\n",
    "    'rescaleFactor': 2,\n",
    "    'base_path': '/scratch1/akrami/Latest_Data/Data',\n",
    "}\n",
    "\n",
    "# Seed and Device Configuration\n",
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "sitk.ProcessObject.SetGlobalDefaultThreader(\"Platform\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imgpath = {}\n",
    "# '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv'\n",
    "#'/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/IXI_train_fold0.csv',\n",
    "#csvpath_trains = ['/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv', '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv']\n",
    "csvpath_trains=['./Data/splits/combined_4datasets.csv']\n",
    "pathBase = '/scratch1/akrami/Data_train'\n",
    "csvpath_val = './Data/splits/IXI_test.csv'\n",
    "csvpath_test = './Data/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "\n",
    "\n",
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = ''\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "  \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=8)\n",
    "test_loader = DataLoader(data_test, batch_size=1,shuffle=False,num_workers=8)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[32, 64, 64],\n",
    "    attention_levels=[False, False,True],\n",
    "    num_head_channels=[0, 0,32],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "model_filename ='/project/ajoshi_27/akrami/3D_lesion_DF/models/model_large_epoch999.pt'\n",
    "model.load_state_dict(torch.load(model_filename))\n",
    "model.eval()\n",
    "\n",
    "def denoise(noised_img,sample_time,scheduler,inferer,model):\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=True):\n",
    "            for t in range(sample_time - 1, -1, -1):\n",
    "                batch_size = noised_img.size(0)  # Get the batch size\n",
    "                t_batch=torch.Tensor((t,)).to(noised_img.device)\n",
    "                t_batch = t_batch.unsqueeze(0).expand(batch_size, -1)  # Expand tensor `t` to have the desired batch size\n",
    "                t_batch = t_batch.to(noised_img.device)[:,0] \n",
    "                model_output = model(noised_img, timesteps=t_batch)\n",
    "                noised_img, _ = scheduler.step(model_output, t, noised_img)\n",
    "            return noised_img\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def inpaint_image(image_array, mask, mean_denoised, model, scheduler, device):\n",
    "    # Convert inputs to tensors and move to the specified device\n",
    "    \n",
    "    mask = mask.to(device)\n",
    "    val_image_masked = image_array.to(device)\n",
    "    val_image_inpainted = torch.randn_like(val_image_masked).to(device)\n",
    "    timesteps = torch.Tensor((999,)).to(device).long()\n",
    "    scheduler.set_timesteps(num_inference_steps=999)\n",
    "    progress_bar = tqdm.tqdm(scheduler.timesteps)\n",
    "\n",
    "    num_resample_steps = 4\n",
    "    with torch.no_grad():\n",
    "        with autocast(enabled=True):\n",
    "            for t in progress_bar:\n",
    "                for u in range(num_resample_steps):\n",
    "                # get the known portion at t-1\n",
    "                    if t > 0:\n",
    "                        noise =  torch.randn_like(val_image_masked).to(device)\n",
    "                        timesteps_prev = torch.Tensor((t - 1,)).to(device).long()\n",
    "                        val_image_inpainted_prev_known = scheduler.add_noise(\n",
    "                            original_samples=val_image_masked, noise=noise, timesteps=timesteps_prev\n",
    "                        )\n",
    "                    else:\n",
    "                        val_image_inpainted_prev_known = val_image_masked\n",
    "                \n",
    "                # perform a denoising step to get the unknown portion at t-1\n",
    "                    if t > 0:\n",
    "                        timesteps = torch.Tensor((t,)).to(device).long()\n",
    "                        model_output = model(val_image_inpainted, timesteps=timesteps)\n",
    "                        val_image_inpainted_prev_unknown, _ = scheduler.step(model_output, t, val_image_inpainted)\n",
    "\n",
    "                    # combine known and unknown using the mask\n",
    "                    val_image_inpainted = torch.where(\n",
    "                        mask == 1, val_image_inpainted_prev_known, val_image_inpainted_prev_unknown\n",
    "                    )\n",
    "\n",
    "                    # perform resampling\n",
    "                    if t > 0 and u < (num_resample_steps - 1):\n",
    "                        # sample x_t from x_t-1\n",
    "                        noise = torch.randn_like(val_image_masked).to(device)\n",
    "                        val_image_inpainted = (\n",
    "                            torch.sqrt(1 - scheduler.betas[t - 1]) * val_image_inpainted\n",
    "                            + torch.sqrt(scheduler.betas[t - 1]) * noise\n",
    "                        )\n",
    "    \n",
    "    return val_image_inpainted\n",
    "\n",
    "sample_time = 500\n",
    "i = 0\n",
    "all_errors = []\n",
    "\n",
    "all_mse = []\n",
    "all_ssim_values=[]\n",
    "model.eval()\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0457f-eb37-4920-861a-8b7952c3f99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_monai",
   "language": "python",
   "name": "working_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
