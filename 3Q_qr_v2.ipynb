{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4f1a3f-3528-4eee-a5d8-f164db1a3025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2vvw5ss2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>0.00668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">note_half_bio</strong> at: <a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/2vvw5ss2' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio/runs/2vvw5ss2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230922_075243-2vvw5ss2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2vvw5ss2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/ajoshi_27/akrami/3D_lesion_DF/wandb/run-20230922_103159-l3yaa7wg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/l3yaa7wg' target=\"_blank\">note_half_bio</a></strong> to <a href='https://wandb.ai/usc_akrami/33_ddpm_bio' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/usc_akrami/33_ddpm_bio' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/usc_akrami/33_ddpm_bio/runs/l3yaa7wg' target=\"_blank\">https://wandb.ai/usc_akrami/33_ddpm_bio/runs/l3yaa7wg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='33_ddpm_bio',name='note_half_bio')\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "from multiprocessing import Manager\n",
    "from typing import Optional\n",
    "\n",
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# MONAI libraries\n",
    "# from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld, \n",
    "    CenterSpatialCropd, \n",
    "    Compose, \n",
    "    Lambdad, \n",
    "    LoadImaged, \n",
    "    Resized, \n",
    "    ScaleIntensityd\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# Other medical image processing libraries\n",
    "import SimpleITK as sitk\n",
    "import torchio as tio\n",
    "\n",
    "# Plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler, DDIMScheduler\n",
    "\n",
    "# Weights and Biases for experiment tracking\n",
    "from dataloader import Train,Eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'batch_size': 4,\n",
    "    'imgDimResize':(160,192,160),\n",
    "    'imgDimPad': (208, 256, 208),\n",
    "    'spatialDims': '3D',\n",
    "    'unisotropic_sampling': True, \n",
    "    'perc_low': 0, \n",
    "    'perc_high': 100,\n",
    "    'rescaleFactor':2,\n",
    "    'base_path': '/scratch1/akrami/Latest_Data/Data',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0755d6b-ac34-457b-9b75-797bd844fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.update(config )\n",
    "\n",
    "\n",
    "imgpath = {}\n",
    "# '/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv'\n",
    "#'/acmenas/hakrami/patched-Diffusion-Models-UAD/Data/splits/IXI_train_fold0.csv',\n",
    "#csvpath_trains = ['/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv', '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/BioBank_train.csv']\n",
    "csvpath_trains=['./combined.csv']\n",
    "pathBase = '/scratch1/akrami/Data_train'\n",
    "csvpath_val = '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/IXI_val_fold0.csv'\n",
    "csvpath_test = '/project/ajoshi_27/akrami/patched-Diffusion-Models-UAD/Data/splits/Brats21_sub_test.csv'\n",
    "var_csv = {}\n",
    "states = ['train','val','test']\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file path and read it into a DataFrame\n",
    "for csvpath in csvpath_trains:\n",
    "    df = pd.read_csv(csvpath)\n",
    "    df_list.append(df)\n",
    "\n",
    "# dfffff=  pd.concat(df_list, ignore_index=True)\n",
    "# dfffff.to_csv(\"./combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3905589c-caa6-4352-a35b-1bd1d1d3e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_csv['train'] =pd.concat(df_list, ignore_index=True)\n",
    "var_csv['val'] = pd.read_csv(csvpath_val)\n",
    "var_csv['test'] = pd.read_csv(csvpath_test)\n",
    "# if cfg.mode == 't2':\n",
    "#     keep_t2 = pd.read_csv(cfg.path.IXI.keep_t2) # only keep t2 images that have a t1 counterpart\n",
    "\n",
    "for state in states:\n",
    "    var_csv[state]['settype'] = state\n",
    "    var_csv[state]['norm_path'] = pathBase  + var_csv[state]['norm_path']\n",
    "    var_csv[state]['img_path'] = pathBase  + var_csv[state]['img_path']\n",
    "    var_csv[state]['mask_path'] = pathBase  + var_csv[state]['mask_path']\n",
    "    if state != 'test':\n",
    "        var_csv[state]['seg_path'] = None\n",
    "    else:\n",
    "        var_csv[state]['seg_path'] = pathBase  + var_csv[state]['seg_path']\n",
    "\n",
    "    # if cfg.mode == 't2': \n",
    "    #     var_csv[state] =var_csv[state][var_csv[state].img_name.isin(keep_t2['0'].str.replace('t2','t1'))]\n",
    "    #     var_csv[state]['img_path'] = var_csv[state]['img_path'].str.replace('t1','t2')\n",
    "    \n",
    "    \n",
    "data_train = Train(var_csv['train'],config) \n",
    "data_val = Train(var_csv['val'],config)                \n",
    "data_test = Eval(var_csv['test'],config)\n",
    "\n",
    "\n",
    "\n",
    "#data_train = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_train_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "train_loader = DataLoader(data_train, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_val = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/IXI_val_fold0.csv', converters={'img_path': pd.eval}), config)\n",
    "val_loader = DataLoader(data_val, batch_size=config.get('batch_size', 1),shuffle=True,num_workers=8)\n",
    "\n",
    "#data_test = Train(pd.read_csv('/project/ajoshi_27/akrami/monai3D/GenerativeModels/data/split/Brats21_test.csv', converters={'img_path': pd.eval}), config)\n",
    "test_loader = DataLoader(data_test, batch_size=config.get('batch_size', 1),shuffle=False,num_workers=8)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=[32, 64, 128, 128],\n",
    "    attention_levels=[False, False, False,True],\n",
    "    num_head_channels=[0, 0, 0,32],\n",
    "    num_res_blocks=2,\n",
    ")\n",
    "model_filename = '/scratch1/akrami/models/3Ddiffusion/half/model_epoch984.pt'\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "n_epochs = 50\n",
    "val_interval = 25\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "\n",
    "\n",
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f66cd3-a109-4b96-8d54-dbf1579cffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"scaled_linear_beta\", beta_start=0.0005, beta_end=0.0195)\n",
    "\n",
    "# inferer = DiffusionInferer(scheduler)\n",
    "\n",
    "# optimizer = torch.optim.Adam(params=model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "# epoch_loss_list = []\n",
    "# val_epoch_loss_list = []\n",
    "\n",
    "# scaler = GradScaler()\n",
    "# total_start = time.time()\n",
    "# n_epochs = config.get('n_epochs',100)\n",
    "# val_interval =config.get('val_interval',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cb78a-1943-4379-912d-a5b5c9c248f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████| 2410/2410 [22:12<00:00,  1.81it/s, loss=0.0538]\n",
      "Epoch 1: 100%|███████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.0104]\n",
      "Epoch 2: 100%|██████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00868]\n",
      "Epoch 3: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00793]\n",
      "Epoch 4: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00732]\n",
      "Epoch 5: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00717]\n",
      "Epoch 6: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00684]\n",
      "Epoch 7: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00661]\n",
      "Epoch 8: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00658]\n",
      "Epoch 9: 100%|██████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00666]\n",
      "Epoch 10: 100%|█████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00659]\n",
      "Epoch 11: 100%|█████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00662]\n",
      "Epoch 12: 100%|█████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00672]\n",
      "Epoch 13: 100%|█████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00618]\n",
      "Epoch 14: 100%|█████| 2410/2410 [13:48<00:00,  2.91it/s, loss=0.00627]\n",
      "Epoch 15: 100%|█████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00647]\n",
      "Epoch 16: 100%|█████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00639]\n",
      "Epoch 17: 100%|█████| 2410/2410 [13:47<00:00,  2.91it/s, loss=0.00618]\n",
      "Epoch 18:  49%|██▍  | 1172/2410 [06:42<06:58,  2.96it/s, loss=0.00597]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile=0.5, reduction='mean'):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        assert preds.size() == target.size()\n",
    "        diff = target - preds\n",
    "        loss = torch.where(diff >= 0, self.quantile * diff, (self.quantile - 1) * diff)\n",
    "        if self.reduction =='mean':\n",
    "            return torch.mean(loss)\n",
    "        else:\n",
    "            return torch.mean(loss, dim=(1, 2, 3))\n",
    "            \n",
    "\n",
    "quantile_loss_l = QuantileLoss(quantile=0.05)\n",
    "quantile_loss_m = QuantileLoss(quantile=0.5)\n",
    "quantile_loss_h = QuantileLoss(quantile=0.95)\n",
    "\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "n_epochs = config.get('n_epochs',100)\n",
    "val_interval =config.get('val_interval',2)\n",
    "\n",
    "wandb.watch(model, log_freq=100)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar:\n",
    "       # images = batch[\"image\"].to(device)\n",
    "        images = batch['vol']['data'].to(device)\n",
    "        images  = torch.rand(1).item()* images\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # Generate random noise\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "\n",
    "            # Create timesteps\n",
    "            timesteps = torch.randint(\n",
    "                0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "            ).long()\n",
    "\n",
    "            # Get model prediction\n",
    "            prediction,prediction_m,prediction_h = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "            loss = quantile_loss_l(prediction, images) + quantile_loss_m(prediction_m, images) + quantile_loss_h(prediction_h, images)\n",
    "\n",
    "            #loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "    wandb.log({\"loss_train\": epoch_loss / (step + 1)},step=epoch)\n",
    "\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            images = batch['vol']['data'].to(device)\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    timesteps = torch.randint(\n",
    "                        0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    prediction,prediction_m,prediction_h = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = quantile_loss_l(prediction, images) + quantile_loss_m(prediction_m, images) + quantile_loss_h(prediction_h, images)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        wandb.log({\"loss_val\": val_epoch_loss / (step + 1)},step=epoch)\n",
    "\n",
    "        # Sampling image during training\n",
    "        #80, 96, 80\n",
    "        image = torch.randn_like(images)[0:1,:,:,:]\n",
    "        image = image.to(device)\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "        timesteps = torch.randint(\n",
    "                        inferer.scheduler.num_train_timesteps-1, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device\n",
    "                    ).long()\n",
    "        with autocast(enabled=True):\n",
    "            prediction,prediction_m,prediction_h = inferer(inputs=image, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "\n",
    "\n",
    "        middle_slice_idx = int(image.size(-1) // 2)\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction_m[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_m\": [wandb.Image(plt)]},step=epoch)\n",
    "        filename = f\"./results/half_norm_3Q_note_v2/sample_epoch{epoch}.png\"\n",
    "        plt.savefig(filename, dpi=300) \n",
    "\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_l\": [wandb.Image(plt)]},step=epoch)\n",
    "\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(prediction_h[0, 0, :, :, middle_slice_idx].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        wandb.log({\"sample_image_h\": [wandb.Image(plt)]},step=epoch)\n",
    "        # Modify the filename to include the epoch number\n",
    "        \n",
    "\n",
    "         \n",
    "        # Save the model\n",
    "        model_filename = f\"/scratch1/akrami/models/3Ddiffusion/half_3Q_note_v2/model_epoch{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ac720-11ef-449b-8331-f1e866774035",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"/scratch1/akrami/models/3Ddiffusion/half_norm/model_epoch{epoch}.pt\"\n",
    "f\"./results/half_norm/sample_epoch{epoch}.png\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_monai",
   "language": "python",
   "name": "working_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
